{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the high level transfer learning APIs, you can easily customize pretrained models for feature extraction or fine-tuning. \n",
    "\n",
    "In this notebook, we will use a pre-trained Inception_V1 model. But we will operate on the pre-trained model to freeze first few layers, replace the classifier on the top, then fine tune the whole model. And we use the fine-tuned model to solve the dogs-vs-cats classification problem,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get the dogs-vs-cats datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the training dataset from https://www.kaggle.com/c/dogs-vs-cats and extract it. \n",
    "\n",
    "The following commands copy about 1100 images of cats and dogs into demo/cats and demo/dogs separately. \n",
    "```shell\n",
    "mkdir -p demo/dogs\n",
    "mkdir -p demo/cats\n",
    "cp train/cat.7* demo/cats\n",
    "cp train/dog.7* demo/dogs```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get the pre-trained Inception-V1 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the pre-trained Inception-V1 model from [Zoo](https://s3-ap-southeast-1.amazonaws.com/bigdl-models/imageclassification/imagenet/bigdl_inception-v1_imagenet_0.4.0.model) \n",
    " Alternatively, user may also download pre-trained caffe/Tensorflow/keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from bigdl.nn.criterion import CrossEntropyCriterion\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "\n",
    "from zoo.common.nncontext import *\n",
    "from zoo.feature.image import *\n",
    "from zoo.pipeline.api.keras.layers import Dense, Input, Flatten\n",
    "from zoo.pipeline.api.keras.models import *\n",
    "from zoo.pipeline.api.net import *\n",
    "from zoo.pipeline.nnframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = init_nncontext(\"ImageTransferLearningExample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manually set model_path and image_path for training\n",
    "\n",
    "1. model_path = path to the pre-trained models. (E.g. path/to/model/bigdl_inception-v1_imagenet_0.4.0.model)\n",
    "\n",
    "2. image_path = path to the folder of the training images. (E.g. path/to/data/dogs-vs-cats/demo/\\*/\\*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_path = \"/opt/work/bigdl_inception-v1_imagenet_0.4.0.model\"\n",
    "image_path = \"file:///opt/work/demo/*/*\"\n",
    "imageDF = NNImageReader.readImages(image_path, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        name|label|\n",
      "+------------+-----+\n",
      "|cat.7859.jpg|  1.0|\n",
      "|cat.7307.jpg|  1.0|\n",
      "|cat.7156.jpg|  1.0|\n",
      "|cat.7718.jpg|  1.0|\n",
      "|cat.7519.jpg|  1.0|\n",
      "|cat.7397.jpg|  1.0|\n",
      "|cat.7134.jpg|  1.0|\n",
      "|cat.7841.jpg|  1.0|\n",
      "|cat.7669.jpg|  1.0|\n",
      "|cat.7518.jpg|  1.0|\n",
      "+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "getName = udf(lambda row:\n",
    "                  re.search(r'(cat|dog)\\.([\\d]*)\\.jpg', row[0], re.IGNORECASE).group(0),\n",
    "                  StringType())\n",
    "getLabel = udf(lambda name: 1.0 if name.startswith('cat') else 2.0, DoubleType())\n",
    "\n",
    "labelDF = imageDF.withColumn(\"name\", getName(col(\"image\"))) \\\n",
    "        .withColumn(\"label\", getLabel(col('name')))\n",
    "(trainingDF, validationDF) = labelDF.randomSplit([0.9, 0.1])\n",
    "labelDF.select(\"name\",\"label\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fine-tune a pre-trained model by removing the last few layers, freezing the first few layers, and adding some new layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createRowToImageFeature\n",
      "creating: createImageResize\n",
      "creating: createImageCenterCrop\n",
      "creating: createImageChannelNormalize\n",
      "creating: createImageMatToTensor\n",
      "creating: createImageFeatureToTensor\n",
      "creating: createChainedPreprocessing\n"
     ]
    }
   ],
   "source": [
    "transformer = ChainedPreprocessing(\n",
    "        [RowToImageFeature(), ImageResize(256, 256), ImageCenterCrop(224, 224),\n",
    "         ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(), ImageFeatureToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Net API to load a pre-trained model, including models saved by Analytics Zoo, BigDL, Torch, Caffe and Tensorflow. Please refer to [Net API Guide](https://analytics-zoo.github.io/master/#APIGuide/PipelineAPI/net/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = Net.load_bigdl(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the last few layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we print all the model layers and you can choose which layer(s) to remove.\n",
    "\n",
    "When a model is loaded using Net, we can use the newGraph(output) api to define a Model with the output specified by the parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "conv1/7x7_s2\n",
      "conv1/relu_7x7\n",
      "pool1/3x3_s2\n",
      "pool1/norm1\n",
      "conv2/3x3_reduce\n",
      "conv2/relu_3x3_reduce\n",
      "conv2/3x3\n",
      "conv2/relu_3x3\n",
      "conv2/norm2\n",
      "pool2/3x3_s2\n",
      "inception_3a/3x3_reduce\n",
      "inception_3a/5x5_reduce\n",
      "inception_3a/relu_3x3_reduce\n",
      "inception_3a/relu_5x5_reduce\n",
      "inception_3a/pool\n",
      "inception_3a/1x1\n",
      "inception_3a/3x3\n",
      "inception_3a/5x5\n",
      "inception_3a/pool_proj\n",
      "inception_3a/relu_pool_proj\n",
      "inception_3a/relu_5x5\n",
      "inception_3a/relu_3x3\n",
      "inception_3a/relu_1x1\n",
      "inception_3a/output\n",
      "inception_3b/3x3_reduce\n",
      "inception_3b/5x5_reduce\n",
      "inception_3b/relu_3x3_reduce\n",
      "inception_3b/relu_5x5_reduce\n",
      "inception_3b/pool\n",
      "inception_3b/1x1\n",
      "inception_3b/3x3\n",
      "inception_3b/5x5\n",
      "inception_3b/pool_proj\n",
      "inception_3b/relu_pool_proj\n",
      "inception_3b/relu_5x5\n",
      "inception_3b/relu_3x3\n",
      "inception_3b/relu_1x1\n",
      "inception_3b/output\n",
      "pool3/3x3_s2\n",
      "inception_4a/3x3_reduce\n",
      "inception_4a/5x5_reduce\n",
      "inception_4a/relu_3x3_reduce\n",
      "inception_4a/relu_5x5_reduce\n",
      "inception_4a/pool\n",
      "inception_4a/1x1\n",
      "inception_4a/3x3\n",
      "inception_4a/5x5\n",
      "inception_4a/pool_proj\n",
      "inception_4a/relu_pool_proj\n",
      "inception_4a/relu_5x5\n",
      "inception_4a/relu_3x3\n",
      "inception_4a/relu_1x1\n",
      "inception_4a/output\n",
      "inception_4b/3x3_reduce\n",
      "inception_4b/5x5_reduce\n",
      "inception_4b/relu_3x3_reduce\n",
      "inception_4b/relu_5x5_reduce\n",
      "inception_4b/pool\n",
      "inception_4b/1x1\n",
      "inception_4b/3x3\n",
      "inception_4b/5x5\n",
      "inception_4b/pool_proj\n",
      "inception_4b/relu_pool_proj\n",
      "inception_4b/relu_5x5\n",
      "inception_4b/relu_3x3\n",
      "inception_4b/relu_1x1\n",
      "inception_4b/output\n",
      "inception_4c/3x3_reduce\n",
      "inception_4c/5x5_reduce\n",
      "inception_4c/relu_3x3_reduce\n",
      "inception_4c/relu_5x5_reduce\n",
      "inception_4c/pool\n",
      "inception_4c/1x1\n",
      "inception_4c/3x3\n",
      "inception_4c/5x5\n",
      "inception_4c/pool_proj\n",
      "inception_4c/relu_pool_proj\n",
      "inception_4c/relu_5x5\n",
      "inception_4c/relu_3x3\n",
      "inception_4c/relu_1x1\n",
      "inception_4c/output\n",
      "inception_4d/3x3_reduce\n",
      "inception_4d/5x5_reduce\n",
      "inception_4d/relu_3x3_reduce\n",
      "inception_4d/relu_5x5_reduce\n",
      "inception_4d/pool\n",
      "inception_4d/1x1\n",
      "inception_4d/3x3\n",
      "inception_4d/5x5\n",
      "inception_4d/pool_proj\n",
      "inception_4d/relu_pool_proj\n",
      "inception_4d/relu_5x5\n",
      "inception_4d/relu_3x3\n",
      "inception_4d/relu_1x1\n",
      "inception_4d/output\n",
      "inception_4e/3x3_reduce\n",
      "inception_4e/5x5_reduce\n",
      "inception_4e/relu_3x3_reduce\n",
      "inception_4e/relu_5x5_reduce\n",
      "inception_4e/pool\n",
      "inception_4e/1x1\n",
      "inception_4e/3x3\n",
      "inception_4e/5x5\n",
      "inception_4e/pool_proj\n",
      "inception_4e/relu_pool_proj\n",
      "inception_4e/relu_5x5\n",
      "inception_4e/relu_3x3\n",
      "inception_4e/relu_1x1\n",
      "inception_4e/output\n",
      "pool4/3x3_s2\n",
      "inception_5a/3x3_reduce\n",
      "inception_5a/5x5_reduce\n",
      "inception_5a/relu_3x3_reduce\n",
      "inception_5a/relu_5x5_reduce\n",
      "inception_5a/pool\n",
      "inception_5a/1x1\n",
      "inception_5a/3x3\n",
      "inception_5a/5x5\n",
      "inception_5a/pool_proj\n",
      "inception_5a/relu_pool_proj\n",
      "inception_5a/relu_5x5\n",
      "inception_5a/relu_3x3\n",
      "inception_5a/relu_1x1\n",
      "inception_5a/output\n",
      "inception_5b/3x3_reduce\n",
      "inception_5b/5x5_reduce\n",
      "inception_5b/relu_3x3_reduce\n",
      "inception_5b/relu_5x5_reduce\n",
      "inception_5b/pool\n",
      "inception_5b/1x1\n",
      "inception_5b/3x3\n",
      "inception_5b/5x5\n",
      "inception_5b/pool_proj\n",
      "inception_5b/relu_pool_proj\n",
      "inception_5b/relu_5x5\n",
      "inception_5b/relu_3x3\n",
      "inception_5b/relu_1x1\n",
      "inception_5b/output\n",
      "pool5/7x7_s1\n",
      "pool5/drop_7x7_s1\n",
      "Viewb5bcd097\n",
      "loss3/classifier\n",
      "prob\n"
     ]
    }
   ],
   "source": [
    "for layer in full_model.layers:\n",
    "    print (layer.name())\n",
    "model = full_model.new_graph([\"pool5/drop_7x7_s1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returning model's output layer is \"pool5/drop_7x7_s1\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze some layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We freeze layers from input to pool4/3x3_s2 inclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_up_to([\"pool4/3x3_s2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a few new layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createZooKerasInput\n",
      "creating: createZooKerasFlatten\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasModel\n",
      "creating: createCrossEntropyCriterion\n",
      "creating: createScalarToTensor\n",
      "creating: createFeatureLabelPreprocessing\n",
      "creating: createNNClassifier\n"
     ]
    }
   ],
   "source": [
    "inputNode = Input(name=\"input\", shape=(3, 224, 224))\n",
    "inception = model.to_keras()(inputNode)\n",
    "flatten = Flatten()(inception)\n",
    "logits = Dense(2)(flatten)\n",
    "lrModel = Model(inputNode, logits)\n",
    "classifier = NNClassifier(lrModel, CrossEntropyCriterion(), transformer) \\\n",
    "        .setLearningRate(0.003).setBatchSize(40).setMaxEpoch(1).setFeaturesCol(\"image\") \\\n",
    "        .setCachingSample(False)\n",
    "pipeline = Pipeline(stages=[classifier])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transfer learning can finish in a few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catdogModel = pipeline.fit(trainingDF)\n",
    "predictionDF = catdogModel.transform(validationDF).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionDF.select(\"name\",\"label\",\"prediction\").sort(\"label\", ascending=False).show(10)\n",
    "predictionDF.select(\"name\",\"label\",\"prediction\").show(10)\n",
    "correct = predictionDF.filter(\"label=prediction\").count()\n",
    "overall = predictionDF.count()\n",
    "accuracy = correct * 1.0 / overall\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model from transfer learning can achieve over 95% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly select some images to show, and print the prediction results here. \n",
    "\n",
    "cat: prediction = 1.0\n",
    "dog: prediction = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplecat=predictionDF.filter(predictionDF.prediction==1.0).limit(3).collect()\n",
    "sampledog=predictionDF.filter(predictionDF.prediction==2.0).sort(\"label\", ascending=False).limit(3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "for cat in samplecat:\n",
    "    print (\"prediction:\"), cat.prediction\n",
    "    display(Image(cat.image.origin[5:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dog in sampledog:\n",
    "    print (\"prediction:\"), dog.prediction\n",
    "    display(Image(dog.image.origin[5:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
