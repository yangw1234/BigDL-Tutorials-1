{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import heapq\n",
    "\n",
    "import tensorflow as tf\n",
    "from zoo import init_nncontext\n",
    "from zoo.pipeline.api.net import TFOptimizer, TFDataset, TFPredictor\n",
    "from bigdl.optim.optimizer import *\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from bigdl.dataset import mnist\n",
    "from bigdl.dataset.transformer import *\n",
    "\n",
    "sys.path.append(\"./slim\")  # add the slim library\n",
    "from nets import lenet\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Extracting', '/tmp/mnist/train-images-idx3-ubyte.gz')\n",
      "('Extracting', '/tmp/mnist/train-labels-idx1-ubyte.gz')\n",
      "('Extracting', '/tmp/mnist/t10k-images-idx3-ubyte.gz')\n",
      "('Extracting', '/tmp/mnist/t10k-labels-idx1-ubyte.gz')\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 1\n",
    "data_num = 60000\n",
    "sc = init_nncontext()\n",
    "\n",
    "\n",
    "# get data, pre-process and create TFDataset\n",
    "def get_data_rdd(dataset):\n",
    "    (images_data, labels_data) = mnist.read_data_sets(\"/tmp/mnist\", dataset)\n",
    "    image_rdd = sc.parallelize(images_data[:data_num])\n",
    "    labels_rdd = sc.parallelize(labels_data[:data_num])\n",
    "    rdd = image_rdd.zip(labels_rdd) \\\n",
    "        .map(lambda rec_tuple: [normalizer(rec_tuple[0], mnist.TRAIN_MEAN, mnist.TRAIN_STD),\n",
    "                                np.array(rec_tuple[1])])\n",
    "    return rdd\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "training_rdd = get_data_rdd(\"train\")\n",
    "testing_rdd = get_data_rdd(\"test\")\n",
    "dataset = TFDataset.from_rdd(training_rdd,\n",
    "                             names=[\"features\", \"labels\"],\n",
    "                             shapes=[[28, 28, 1], []],\n",
    "                             types=[tf.float32, tf.int32],\n",
    "                             batch_size=1024,\n",
    "                             val_rdd=testing_rdd\n",
    "                             )\n",
    "\n",
    "# construct the model from TFDataset\n",
    "images, labels = dataset.tensors\n",
    "\n",
    "with slim.arg_scope(lenet.lenet_arg_scope()):\n",
    "    logits, end_points = lenet.lenet(images, num_classes=10, is_training=True)\n",
    "\n",
    "loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createAdam\n",
      "creating: createTop1Accuracy\n",
      "INFO:tensorflow:Froze 8 variables.\n",
      "INFO:tensorflow:Converted 8 variables to const ops.\n",
      "creating: createTFTrainingHelper\n",
      "creating: createIdentityCriterion\n",
      "creating: createMaxEpoch\n",
      "creating: createDistriOptimizer\n",
      "creating: createTFValidationMethod\n",
      "creating: createEveryEpoch\n",
      "creating: createTrainSummary\n",
      "creating: createValidationSummary\n",
      "creating: createMaxEpoch\n",
      "WARNING:tensorflow:Issue encountered when serializing features:0.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "TFDataset instance has no attribute 'name'\n",
      "WARNING:tensorflow:Issue encountered when serializing labels:0.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "TFDataset instance has no attribute 'name'\n",
      "CPU times: user 2.52 s, sys: 201 ms, total: 2.73 s\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a optimizer\n",
    "optimizer = TFOptimizer(loss, Adam(1e-3),\n",
    "                        val_outputs=[logits],\n",
    "                        val_labels=[labels],\n",
    "                        val_method=Top1Accuracy())\n",
    "optimizer.set_train_summary(TrainSummary(\"/tmp/az_lenet\", \"lenet\"))\n",
    "optimizer.set_val_summary(ValidationSummary(\"/tmp/az_lenet\", \"lenet\"))\n",
    "# kick off training\n",
    "optimizer.optimize(end_trigger=MaxEpoch(max_epoch))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(optimizer.sess, \"/tmp/lenet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/lenet/\n",
      "INFO:tensorflow:Froze 8 variables.\n",
      "INFO:tensorflow:Converted 8 variables to const ops.\n",
      "creating: createTFNet\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "u\"NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: ToInt32 = Cast[DstT=DT_INT32, SrcT=DT_INT64, Truncate=false](ArgMax). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1e0391b482a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/tmp/lenet/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/spark-8d256913-0967-4393-9ac9-8ef5e50188fa/userFiles-a83b93d6-be85-4c5e-b89b-24777918459c/analytics-zoo-bigdl_0.6.0-spark_2.3.1-0.3.0-python-api.zip/zoo/pipeline/api/net.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess, outputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0m_check_the_same\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_required_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_per_thread\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             raise ValueError(\"You should set batch_per_thread on TFDataset\" +\n",
      "\u001b[0;32m/tmp/spark-8d256913-0967-4393-9ac9-8ef5e50188fa/userFiles-a83b93d6-be85-4c5e-b89b-24777918459c/analytics-zoo-bigdl_0.6.0-spark_2.3.1-0.3.0-python-api.zip/zoo/pipeline/api/net.py\u001b[0m in \u001b[0;36mfrom_session\u001b[0;34m(sess, inputs, outputs, generate_backward, allow_non_differentiable_input)\u001b[0m\n\u001b[1;32m    296\u001b[0m             export_tf(sess, temp, inputs, outputs,\n\u001b[1;32m    297\u001b[0m                       generate_backward, allow_non_differentiable_input)\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_export_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/spark-8d256913-0967-4393-9ac9-8ef5e50188fa/userFiles-a83b93d6-be85-4c5e-b89b-24777918459c/analytics-zoo-bigdl_0.6.0-spark_2.3.1-0.3.0-python-api.zip/zoo/pipeline/api/net.py\u001b[0m in \u001b[0;36mfrom_export_folder\u001b[0;34m(folder)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mTFNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/spark-8d256913-0967-4393-9ac9-8ef5e50188fa/userFiles-a83b93d6-be85-4c5e-b89b-24777918459c/analytics-zoo-bigdl_0.6.0-spark_2.3.1-0.3.0-python-api.zip/zoo/pipeline/api/net.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, input_names, output_names, bigdl_type)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moutput_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             super(TFNet, self).__init__(None, bigdl_type,\n\u001b[0;32m--> 221\u001b[0;31m                                         path)\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/spark-8d256913-0967-4393-9ac9-8ef5e50188fa/userFiles-a83b93d6-be85-4c5e-b89b-24777918459c/analytics-zoo-bigdl_0.6.0-spark_2.3.1-0.3.0-python-api.zip/bigdl/nn/layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, jvalue, bigdl_type, *args)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             self.value = callBigDlFunc(\n\u001b[0;32m--> 131\u001b[0;31m                 bigdl_type, self.jvm_class_constructor(), *args)\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigdl_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigdl_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/spark-8d256913-0967-4393-9ac9-8ef5e50188fa/userFiles-a83b93d6-be85-4c5e-b89b-24777918459c/analytics-zoo-bigdl_0.6.0-spark_2.3.1-0.3.0-python-api.zip/bigdl/util/common.py\u001b[0m in \u001b[0;36mcallBigDlFunc\u001b[0;34m(bigdl_type, name, *args)\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"does not exist\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: u\"NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: ToInt32 = Cast[DstT=DT_INT32, SrcT=DT_INT64, Truncate=false](ArgMax). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\""
     ]
    }
   ],
   "source": [
    "# construct the model from TFDataset\n",
    "tf.reset_default_graph()\n",
    "dataset = TFDataset.from_rdd(testing_rdd,\n",
    "                                 names=[\"features\", \"labels\"],\n",
    "                                 shapes=[[28, 28, 1], [1]],\n",
    "                                 types=[tf.float32, tf.int32],\n",
    "                                 batch_per_thread=20\n",
    "                                 )\n",
    "images, labels = dataset.tensors\n",
    "\n",
    "labels = tf.squeeze(labels)\n",
    "\n",
    "with slim.arg_scope(lenet.lenet_arg_scope()):\n",
    "    logits, end_points = lenet.lenet(images, num_classes=10, is_training=False)\n",
    "\n",
    "predictions = tf.to_int32(tf.argmax(logits, axis=1))\n",
    "correct = tf.expand_dims(tf.to_int32(tf.equal(predictions, labels)), axis=1)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, \"/tmp/lenet/\")\n",
    "\n",
    "    predictor = TFPredictor(sess, [correct])\n",
    "\n",
    "    accuracy = predictor.predict().mean()\n",
    "\n",
    "    print(\"predict accuracy is %s\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
