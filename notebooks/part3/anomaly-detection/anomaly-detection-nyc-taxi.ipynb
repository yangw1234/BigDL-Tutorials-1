{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Using RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection detects data points in data that does not fit well with the rest of data. In this notebook we demonstrate how to do unsupervised anomaly detection using recurrent nueral network (RNN) model.\n",
    "\n",
    "We used one of the dataset in Numenta Anomaly Benchmark (NAB) ([link](https://github.com/numenta/NAB)) for demo, i.e. NYC taxi passengers dataset, which contains 10320 records, each indicating the total number of taxi passengers in NYC at a corresonponding time spot. We use RNN to learn from 50 previous values, and predict just the 1 next value. The data points whose actual values are distant from predicted values are considered anomalies (distance threshold can be adjusted as needed).\n",
    "  \n",
    "References: \n",
    "* Unsupervised real-time anomaly detection for streaming data ([link](https://www.sciencedirect.com/science/article/pii/S0925231217309864))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* import necesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from zoo.common.nncontext import *\n",
    "sc = init_nncontext(\"Anomaly Detection Example\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%pylab inline\n",
    "import seaborn\n",
    "import matplotlib.dates as md\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from zoo.pipeline.api.keras.layers import Dense, Dropout, LSTM\n",
    "from zoo.pipeline.api.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-11-06 23:50:22--  https://raw.githubusercontent.com/numenta/NAB/master/data/realKnownCause/nyc_taxi.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.128.133, 151.101.192.133, 151.101.0.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.128.133|:443... failed: Connection timed out.\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.192.133|:443... failed: Connection timed out.\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... failed: Connection timed out.\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.64.133|:443... failed: Connection timed out.\n",
      "Retrying.\n",
      "\n",
      "--2018-11-06 23:59:07--  (try: 2)  https://raw.githubusercontent.com/numenta/NAB/master/data/realKnownCause/nyc_taxi.csv\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.128.133|:443... failed: Connection timed out.\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.192.133|:443... failed: Connection timed out.\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... "
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/numenta/NAB/master/data/realKnownCause/nyc_taxi.csv\n",
    "try:\n",
    "    dataset_path = \"/opt/work/nyc_taxi.csv\"\n",
    "    df = pd.read_csv(dataset_path)\n",
    "except Exception as e:\n",
    "    print(\"nyc_taxi.csv doesn't exist\")\n",
    "    print(\"you can run $ANALYTICS_ZOO_HOME/bin/data/NAB/nyc_taxi/get_nyc_taxi.sh to download nyc_taxi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Understand the data.\n",
    "\n",
    "Each record is in format of (timestamp, value). Timestamps range between 2014-07-01 and 2015-01-31. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the timestamp format and frequence \n",
    "print(df['timestamp'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the mean of passenger number \n",
    "print(df['value'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the type of timestamp column for plotting\n",
    "df['datetime'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# visualisation of anomaly throughout time (viz 1)\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.plot(df['datetime'], df['value'], color='blue', linewidth=0.6)\n",
    "ax.set_title('NYC taxi passengers throughout time')\n",
    "\n",
    "plt.xlabel('datetime')\n",
    "plt.xticks(rotation=45) \n",
    "plt.ylabel('The Number of NYC taxi passengers')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extracting some useful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hours when people are awake (6:00-00:00)\n",
    "df['hours'] = df['datetime'].dt.hour\n",
    "df['awake'] = (((df['hours'] >= 6) & (df['hours'] <= 23)) | (df['hours'] == 0)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of 2 distinct categories that seem useful (sleeping time and awake time)\n",
    "df['categories'] = df['awake']\n",
    "\n",
    "a = df.loc[df['categories'] == 0, 'value']\n",
    "b = df.loc[df['categories'] == 1, 'value']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "a_heights, a_bins = np.histogram(a)\n",
    "b_heights, b_bins = np.histogram(b, bins=a_bins)\n",
    "\n",
    "width = (a_bins[1] - a_bins[0])/6\n",
    "\n",
    "ax.bar(a_bins[:-1], a_heights*100/a.count(), width=width, facecolor='yellow', label='Sleeping time')\n",
    "ax.bar(b_bins[:-1]+width, (b_heights*100/b.count()), width=width, facecolor='red', label ='Awake time')\n",
    "ax.set_title('Histogram of NYC taxi passengers in different categories')\n",
    "\n",
    "plt.xlabel('The number of NYC taxi passengers')\n",
    "plt.ylabel('Record counts')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['awake'].head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'].head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result, we can conclude:\n",
    "- more people take taxi when they are awake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Standardizing data and spliting them into the train data and the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select and standardize data\n",
    "data_n = df[['value', 'hours', 'awake']]\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(data_n)\n",
    "data_n = pd.DataFrame(np_scaled)\n",
    "\n",
    "#important parameters and train/test size\n",
    "prediction_time = 1 \n",
    "testdatasize = 1000\n",
    "unroll_length = 50\n",
    "testdatacut = testdatasize + unroll_length  + 1\n",
    "\n",
    "#train data\n",
    "x_train = data_n[0:-prediction_time-testdatacut].as_matrix()\n",
    "y_train = data_n[prediction_time:-testdatacut  ][0].as_matrix()\n",
    "\n",
    "#test data\n",
    "x_test = data_n[0-testdatacut:-prediction_time].as_matrix()\n",
    "y_test = data_n[prediction_time-testdatacut:  ][0].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unroll data: for the data point at index i, create a sequence from i to (i+unroll_length)\n",
    "\n",
    "for example, if unroll_length=5\n",
    "\n",
    "[[1]\n",
    "\n",
    " [2]\n",
    " \n",
    " [3]\n",
    " \n",
    " [4]\n",
    " \n",
    " [5]\n",
    " \n",
    " [6]\n",
    " \n",
    " [7]\n",
    " \n",
    " [8]\n",
    " \n",
    " [9]\n",
    " \n",
    " [10]\n",
    " \n",
    "...\n",
    "\n",
    "]\n",
    "\n",
    "will be unrolled to create following sequences\n",
    "\n",
    "[[ 1,  2,  3,  4,  5]\n",
    "\n",
    " [ 2,  3,  4,  5,  6]\n",
    " \n",
    " [ 3,  4,  5,  6,  7]\n",
    " \n",
    " [ 4,  5,  6,  7,  8]\n",
    "\n",
    " [ 5,  6,  7,  8,  9]\n",
    " \n",
    " [ 6,  7,  8,  9,  10]\n",
    " \n",
    " ...\n",
    " \n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unroll: create sequence of 50 previous data points for each data points\n",
    "def unroll(data,sequence_length=24):\n",
    "    result = []\n",
    "    for index in range(len(data) - sequence_length):\n",
    "        result.append(data[index: index + sequence_length])\n",
    "    return np.asarray(result)\n",
    "\n",
    "# adapt the datasets for the sequence data shape\n",
    "x_train = unroll(x_train,unroll_length)\n",
    "x_test  = unroll(x_test,unroll_length)\n",
    "y_train = y_train[-x_train.shape[0]:]\n",
    "y_test  = y_test[-x_test.shape[0]:]\n",
    "\n",
    "# see the shape\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we show an example of building a RNN network using Analytics Zoo Keras-Style API. \n",
    "There are three LSTM layers and one Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(\n",
    "    input_shape=(x_train.shape[1], x_train.shape[-1]),\n",
    "    output_dim=20,\n",
    "    return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(\n",
    "    10,\n",
    "    return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(\n",
    "    output_dim=1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "print(\"Training begins.\")\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=1024,\n",
    "    nb_epoch=20)\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "* BigDL models make inferences based on the given data using model.predict(val_rdd) API. A result of RDD is returned. predict_class returns the predicted points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of difference between prediction and test data\n",
    "diff=[]\n",
    "ratio=[]\n",
    "predictions = model.predict(x_test)\n",
    "p = predictions.collect()\n",
    "for u in range(len(y_test)):\n",
    "    pr = p[u][0]\n",
    "    ratio.append((y_test[u]/pr)-1)\n",
    "    diff.append(abs(y_test[u]- pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* plot the prediction and the reality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predicted values and actual values (for the test data)\n",
    "fig, axs = plt.subplots()\n",
    "\n",
    "axs.plot(p,color='red', label='predicted values')\n",
    "axs.plot(y_test,color='blue', label='actual values')\n",
    "axs.set_title('the predicted values and actual values (for the test data)')\n",
    "\n",
    "plt.xlabel('test data index')\n",
    "plt.ylabel('number of taxi passengers after min_max_scalar')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set the distance thresold for anomalies. There're many ways to select this threshold. Here we set the expected proportion of anomalies among the entire set. Then we set the threshold as the minimum value of top N distances (here N is the total number of anomalies, i.e. anomaly fraction * total no. of samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An estimation of anomly population of the dataset\n",
    "outliers_fraction = 0.01\n",
    "# select the most distant prediction/reality data points as anomalies\n",
    "diff = pd.Series(diff)\n",
    "number_of_outliers = int(outliers_fraction*len(diff))\n",
    "threshold = diff.nlargest(number_of_outliers).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* plot anomalies in the test data throughout time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the difference and the threshold (for the test data)\n",
    "fig, axs = plt.subplots()\n",
    "\n",
    "axs.plot(diff,color='blue', label='diff')\n",
    "axs.set_title('the difference between the predicted values and actual values with the threshold line')\n",
    "\n",
    "plt.hlines(threshold, 0, 1000, color='red', label='threshold')\n",
    "plt.xlabel('test data index')\n",
    "plt.ylabel('difference value after min_max_scalar')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with anomaly label (test data part)\n",
    "test = (diff >= threshold).astype(int)\n",
    "# the training data part where we didn't predict anything (overfitting possible): no anomaly\n",
    "complement = pd.Series(0, index=np.arange(len(data_n)-testdatasize))\n",
    "last_train_data= (df['datetime'].tolist())[-testdatasize]\n",
    "# add the data to the main\n",
    "df['anomaly27'] = complement.append(test, ignore_index='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* plot anomalies in the test data throughout time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualisation of anomaly throughout time (viz 1)\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "a = df.loc[df['anomaly27'] == 1, ['datetime', 'value']] #anomaly\n",
    "ax.plot(df['datetime'], df['value'], color='blue', label='no anomaly value', linewidth=0.6)\n",
    "ax.scatter(a['datetime'].tolist(),a['value'], color='red', label='anomalies value')\n",
    "ax.set_title('the number of nyc taxi value throughout time (with anomalies scattered)')\n",
    "\n",
    "max_value = df['value'].max()\n",
    "min_value = df['value'].min()\n",
    "plt.vlines(last_train_data, min_value, max_value, color='black', linestyles = \"dashed\", label='test begins')\n",
    "plt.xlabel('datetime')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('the number of nyc taxi value')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
